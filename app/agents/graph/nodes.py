"""
LangGraph Node êµ¬í˜„
RAG Agentë¥¼ ìœ„í•œ ë…¸ë“œë“¤
"""
import logging
from typing import Dict, Any
from ..graph.state import RAGAgentState, AgentStatus

logger = logging.getLogger(__name__)


def log_node_execution(node_name: str, state: RAGAgentState):
    """
    ë…¸ë“œ ì‹¤í–‰ ë¡œê¹… í—¬í¼ í•¨ìˆ˜

    Args:
        node_name: ë…¸ë“œ ì´ë¦„
        state: í˜„ì¬ ìƒíƒœ
    """
    logger.info(f"ğŸ”„ {node_name} ë…¸ë“œ ì‹¤í–‰ - ì§„í–‰ë¥ : {state.get('progress', 0)}%")
    if state.get("error"):
        logger.warning(f"âš ï¸ {node_name} ë…¸ë“œì— ì—ëŸ¬ ìƒíƒœ ì „ë‹¬: {state['error']}")


class RAGAgentNodes:
    """
    RAG Agent ë…¸ë“œë“¤
    ê° ë…¸ë“œëŠ” íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  Stateë¥¼ ì—…ë°ì´íŠ¸
    """

    @staticmethod
    def initialize_node(state: RAGAgentState) -> RAGAgentState:
        """
        ì´ˆê¸°í™” ë…¸ë“œ
        Agent ì‹¤í–‰ì„ ì¤€ë¹„í•˜ê³  ë©”ëª¨ë¦¬ë¥¼ ë¡œë“œ
        """
        log_node_execution("ì´ˆê¸°í™”", state)

        try:
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state["status"] = AgentStatus.INITIALIZING
            state["current_step"] = "Agent ì´ˆê¸°í™” ì¤‘"
            state["progress"] = 10

            # ë©”ëª¨ë¦¬ í‚¤ ì„¤ì • (ê¸°ë³¸ê°’ ë˜ëŠ” conversation_id ì‚¬ìš©)
            memory_key = state.get("conversation_id", "default")
            state["memory_key"] = memory_key

            # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ
            from ...memory import get_conversation_memory

            memory = get_conversation_memory(memory_key)
            memory_variables = memory.load_memory_variables({})

            # íˆìŠ¤í† ë¦¬ í˜•ì‹ ë³€í™˜
            formatted_history = []
            if memory_variables:
                history_text = memory_variables.get(memory_key, "")
                if history_text:
                    # ë©”ëª¨ë¦¬ í…ìŠ¤íŠ¸ íŒŒì‹±: "Human: ...\nAI: ..." í˜•ì‹
                    entries = history_text.split('\n\n')  # ê° ëŒ€í™” ìŒ ë¶„ë¦¬
                    for entry in entries:
                        if entry.strip():
                            lines = entry.strip().split('\n')
                            if len(lines) >= 2:
                                human_line = lines[0].strip()
                                ai_line = lines[1].strip()

                                # Humanê³¼ AI ë¶€ë¶„ ì¶”ì¶œ
                                human = human_line.replace("Human:", "").strip() if human_line.startswith("Human:") else human_line
                                ai = ai_line.replace("AI:", "").strip() if ai_line.startswith("AI:") else ai_line

                                if human and ai:  # ë‘˜ ë‹¤ ë‚´ìš©ì´ ìˆì–´ì•¼ ì¶”ê°€
                                    formatted_history.append({"human": human, "ai": ai})

            state["conversation_history"] = formatted_history
            state["progress"] = 20

            logger.info(f"âœ… Agent ì´ˆê¸°í™” ì™„ë£Œ - ë©”ëª¨ë¦¬ í‚¤: {memory_key}, íˆìŠ¤í† ë¦¬: {len(formatted_history)}ê°œ")

        except Exception as e:
            logger.error(f"âŒ Agent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            state["error"] = str(e)
            state["status"] = AgentStatus.FAILED

        return state

    @staticmethod
    def retrieve_node(state: RAGAgentState) -> RAGAgentState:
        """
        ê²€ìƒ‰ ë…¸ë“œ
        FAISS VectorDBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰
        """
        log_node_execution("ë¬¸ì„œ ê²€ìƒ‰", state)

        try:
            from ...core.vector_db import get_vector_db

            question = state["question"]
            vector_db = get_vector_db()

            # ì‹¤ì œ ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            search_results = vector_db.similarity_search(
                query=question,
                k=5,  # ìµœëŒ€ 5ê°œ ë¬¸ì„œ
                score_threshold=0.0  # ëª¨ë“  ê²°ê³¼ í¬í•¨
            )

            if search_results:
                # ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
                retrieved_docs = []
                context_parts = []

                for doc, score in search_results:
                    doc_info = {
                        "content": doc.page_content,
                        "source": doc.metadata.get("source", "unknown"),
                        "score": float(score),
                        "chunk_id": doc.metadata.get("chunk_id", 0),
                        "topic": doc.metadata.get("topic", "unknown")
                    }
                    retrieved_docs.append(doc_info)
                    context_parts.append(doc.page_content)

                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                state["retrieved_docs"] = retrieved_docs
                state["context"] = "\n\n".join(context_parts)

                logger.info(f"âœ… ì‹¤ì œ VectorDB ê²€ìƒ‰ ì™„ë£Œ: {len(retrieved_docs)}ê°œ ë¬¸ì„œ")
            else:
                # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
                search_results = [
                    {
                        "content": "ë°˜ë„ì²´ ì œì¡° ê³µì •ì€ í¬ê²Œ 8ë‹¨ê³„ë¡œ ë‚˜ë‰©ë‹ˆë‹¤: ì›¨ì´í¼ ì œì¡°, ì‚°í™”, í¬í† ë¦¬ì†Œê·¸ë˜í”¼, ì‹ê°, ì´ì˜¨ì£¼ì…, ê¸ˆì†í™”, íŒ¨ì‹œë² ì´ì…˜, íŒ¨í‚¤ì§•.",
                        "source": "semiconductor_fundamentals.pdf",
                        "score": 0.95
                    }
                ]
                state["retrieved_docs"] = search_results
                state["context"] = search_results[0]["content"]
                logger.warning("âš ï¸ VectorDB ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©")

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state["status"] = AgentStatus.RETRIEVING
            state["current_step"] = "ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘"
            state["progress"] = 50

            logger.info(f"âœ… ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ë¬¸ì„œ")

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            state["error"] = str(e)
            state["status"] = AgentStatus.FAILED

        return state

    @staticmethod
    def generate_node(state: RAGAgentState) -> RAGAgentState:
        """
        ìƒì„± ë…¸ë“œ
        ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì™€ ë©”ëª¨ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µ ë‹µë³€ ìƒì„±
        """
        log_node_execution("ë‹µë³€ ìƒì„±", state)

        try:
            # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í™œìš©
            from ...memory import get_conversation_memory

            memory_key = state.get("memory_key", "default")
            memory = get_conversation_memory(memory_key)

            # ë©”ëª¨ë¦¬ì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
            memory_variables = memory.load_memory_variables({"input": state["question"]})
            memory_context = memory_variables.get(memory_key, "")

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ë„ í™œìš©
            conversation_context = ""
            if state.get("conversation_history"):
                recent_history = state["conversation_history"][-3:]  # ìµœê·¼ 3ê°œ
                conversation_context = "\n".join([
                    f"ì´ì „ ëŒ€í™”: Human: {h.get('human', '')} | AI: {h.get('ai', '')}"
                    for h in recent_history
                ])

            # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í†µí•©
            full_memory_context = ""
            if memory_context:
                full_memory_context += f"ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸:\n{memory_context}\n\n"
            if conversation_context:
                full_memory_context += f"ëŒ€í™” íˆìŠ¤í† ë¦¬:\n{conversation_context}"

            memory_context = full_memory_context

            # ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ í™œìš©
            search_context = state.get("context", "")
            question = state["question"]

            # ì‹¬ì¸µ ë‹µë³€ ìƒì„± (ì‹¤ì œë¡œëŠ” Ollamaë¡œ ìƒì„±)
            if "ì•ˆë…•" in question or "hello" in question.lower():
                answer = "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë°˜ë„ì²´ ì œì¡°(VirtualFab/Digital Twin) ë¶„ì•¼ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë°˜ë„ì²´ ê³µì •, Digital Twin ê¸°ìˆ , ê³µì • ìµœì í™” ë“±ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!"
            elif any(keyword in question for keyword in ["ë°˜ë„ì²´", "semiconductor", "ê³µì •", "process"]):
                answer = f"ë°˜ë„ì²´ ì œì¡° ê³µì •ì— ëŒ€í•´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n{search_context}\n\nì´ ì™¸ì— ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
            elif any(keyword in question for keyword in ["virtualfab", "digital twin", "ê°€ìƒê³µì¥"]):
                answer = f"VirtualFabê³¼ Digital Twin ê¸°ìˆ ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n{search_context}\n\nì´ ê¸°ìˆ ë“¤ì€ ë°˜ë„ì²´ ì œì¡°ì˜ íš¨ìœ¨ì„±ê³¼ í’ˆì§ˆ í–¥ìƒì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤."
            else:
                answer = f"ê·€í•˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n{search_context}\n\në” ìì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•˜ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”."

            # ê²°ê³¼ ì €ì¥
            state["answer"] = answer
            state["sources"] = state.get("retrieved_docs", [])
            state["metadata"] = {
                "llm_provider": "llamacpp",
                "model": "LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF",
                "context_used": bool(search_context),
                "memory_used": bool(memory_context)
            }

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state["status"] = AgentStatus.GENERATING
            state["current_step"] = "ë‹µë³€ ìƒì„± ì¤‘"
            state["progress"] = 80

            logger.info("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            state["error"] = str(e)
            state["status"] = AgentStatus.FAILED

        return state

    @staticmethod
    def finalize_node(state: RAGAgentState) -> RAGAgentState:
        """
        ë§ˆë¬´ë¦¬ ë…¸ë“œ
        ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ê³  ë©”ëª¨ë¦¬ì— ì €ì¥
        """
        log_node_execution("ë§ˆë¬´ë¦¬", state)

        try:
            from ...memory import get_conversation_memory

            # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
            memory = get_conversation_memory(state["memory_key"])
            memory.save_context(
                inputs={"human": state["question"]},
                outputs={"ai": state["answer"]}
            )

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            conversation_entry = {
                "human": state["question"],
                "ai": state["answer"],
                "timestamp": state.get("timestamp", None)
            }
            state["conversation_history"].append(conversation_entry)

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state["status"] = AgentStatus.COMPLETED
            state["current_step"] = "ì²˜ë¦¬ ì™„ë£Œ"
            state["progress"] = 100

            logger.info("âœ… Agent ì‹¤í–‰ ì™„ë£Œ (ë©”ëª¨ë¦¬ ì €ì¥ë¨)")

        except Exception as e:
            logger.error(f"âŒ ë§ˆë¬´ë¦¬ ì‹¤íŒ¨: {e}")
            state["error"] = str(e)
            state["status"] = AgentStatus.FAILED

        return state

    @staticmethod
    def error_handler_node(state: RAGAgentState) -> RAGAgentState:
        """
        ì—ëŸ¬ í•¸ë“¤ëŸ¬ ë…¸ë“œ
        ì˜¤ë¥˜ ë°œìƒ ì‹œ ì²˜ë¦¬
        """
        error_msg = state.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
        logger.error(f"ğŸš¨ Agent ì—ëŸ¬ í•¸ë“¤ëŸ¬ ì‹¤í–‰: {error_msg}")

        state["status"] = AgentStatus.FAILED
        state["answer"] = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"
        state["progress"] = 100
        state["current_step"] = "ì˜¤ë¥˜ ì²˜ë¦¬ ì™„ë£Œ"

        return state


# ë…¸ë“œ í•¨ìˆ˜ë“¤ (LangGraphì—ì„œ ì‚¬ìš©)
initialize_agent = RAGAgentNodes.initialize_node
retrieve_documents = RAGAgentNodes.retrieve_node
generate_answer = RAGAgentNodes.generate_node
finalize_agent = RAGAgentNodes.finalize_node
handle_error = RAGAgentNodes.error_handler_node
