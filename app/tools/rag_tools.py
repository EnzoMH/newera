"""
RAG ê´€ë ¨ Tools
ë²¡í„° ê²€ìƒ‰, ë¬¸ì„œ ì²˜ë¦¬ ë“± RAG ì „ìš© Tools
"""
import logging
from typing import Any, Dict, Optional
from langchain.tools import BaseTool
from langchain.callbacks.manager import CallbackManagerForToolRun

logger = logging.getLogger(__name__)


class VectorSearchTool(BaseTool):
    """
    ë²¡í„° ê²€ìƒ‰ Tool
    FAISS ë˜ëŠ” Chromaë¥¼ í†µí•œ ìœ ì‚¬ë„ ê²€ìƒ‰
    """

    name = "vector_search"
    description = "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì§ˆë¬¸ì´ë‚˜ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”."

    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """
        ë™ê¸° ê²€ìƒ‰ ì‹¤í–‰
        """
        try:
            logger.info(f"ğŸ” ì‹¤ì œ ë²¡í„° ê²€ìƒ‰: {query}")

            # ì‹¤ì œ VectorDB ì‚¬ìš©
            from ..core.vector_db import get_vector_db
            
            vector_db = get_vector_db()
            results = vector_db.similarity_search(query, k=5)

            if not results:
                return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. VectorDBì— ë¬¸ì„œê°€ ì¶”ê°€ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

            result_text = f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):\n"
            for i, result in enumerate(results, 1):
                content = result.get('content', result.get('page_content', ''))[:150] + "..."
                score = result.get('score', 'N/A')
                source = result.get('metadata', {}).get('source', 'Unknown')
                result_text += f"{i}. [{source}] {content} (ìœ ì‚¬ë„: {score})\n"

            return result_text

        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def _arun(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """
        ë¹„ë™ê¸° ê²€ìƒ‰ ì‹¤í–‰
        """
        # í˜„ì¬ëŠ” ë™ê¸°ì™€ ë™ì¼í•œ ë¡œì§
        return self._run(query, run_manager)


class DocumentChunkerTool(BaseTool):
    """
    ë¬¸ì„œ ì²­í‚¹ Tool
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• 
    """

    name = "document_chunker"
    description = "ê¸´ ë¬¸ì„œë¥¼ ì§€ì •ëœ í¬ê¸°ë¡œ ì²­í‚¹í•©ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ê³¼ ì²­í¬ í¬ê¸°ë¥¼ ì…ë ¥í•˜ì„¸ìš”."

    def _run(self, input_text: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """
        ë™ê¸° ì²­í‚¹ ì‹¤í–‰
        """
        try:
            logger.info("ğŸ“„ ë¬¸ì„œ ì²­í‚¹ ì‹œì‘")

            # LangChain RecursiveCharacterTextSplitter ì‚¬ìš©
            from langchain.text_splitter import RecursiveCharacterTextSplitter

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", ". ", " ", ""],
                length_function=len,
            )

            # íŒŒì¼ ê²½ë¡œì¸ì§€ í™•ì¸ í›„ ì½ê¸°
            content = input_text
            if input_text.endswith(('.txt', '.md', '.py', '.js', '.json', '.pdf')):
                if input_text.endswith('.pdf'):
                    # PDFëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”
                    return "PDF íŒŒì¼ì€ PDF Toolì„ ì‚¬ìš©í•˜ì„¸ìš”."
                try:
                    with open(input_text, 'r', encoding='utf-8') as f:
                        content = f.read()
                except Exception as e:
                    return f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}"

            chunks = text_splitter.split_text(content)

            result = f"LangChain ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±\n"
            result += f"- ì›ë³¸ ê¸¸ì´: {len(content):,} ë¬¸ì\n"
            result += f"- ì²­í¬ í¬ê¸°: ìµœëŒ€ 1000ì (ì˜¤ë²„ë© 200ì)\n\n"

            for i, chunk in enumerate(chunks[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                preview = chunk[:100] + "..." if len(chunk) > 100 else chunk
                result += f"ì²­í¬ {i}: {preview}\n\n"

            if len(chunks) > 3:
                result += f"... ì™¸ {len(chunks) - 3}ê°œ ì²­í¬"

            return result

        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²­í‚¹ ì‹¤íŒ¨: {e}")
            return f"ì²­í‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def _arun(self, input_text: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """
        ë¹„ë™ê¸° ì²­í‚¹ ì‹¤í–‰
        """
        return self._run(input_text, run_manager)


class ContextRetrieverTool(BaseTool):
    """
    ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ Tool
    ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘
    """

    name = "context_retriever"
    description = "ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤."

    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """
        ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        """
        try:
            logger.info(f"ğŸ” ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰: {query}")

            # TODO: ì‹¤ì œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ êµ¬í˜„
            # ë²¡í„° ê²€ìƒ‰ + ë©”ëª¨ë¦¬ ê²€ìƒ‰ ë“± í†µí•©

            dummy_contexts = [
                "ë°˜ë„ì²´ ì œì¡° ê³µì •ì€ 8ê°œì˜ ì£¼ìš” ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.",
                "Digital Twinì€ ë¬¼ë¦¬ì  ì‹œìŠ¤í…œì˜ ê°€ìƒ ë³µì œë³¸ì…ë‹ˆë‹¤.",
                "Virtual MetrologyëŠ” ì¸¡ì • ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤."
            ]

            result = f"ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼:\n\n"
            for i, context in enumerate(dummy_contexts, 1):
                result += f"{i}. {context}\n"

            return result

        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return f"ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def _arun(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """
        ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        """
        return self._run(query, run_manager)


class PDFProcessorTool(BaseTool):
    """
    PDF ì²˜ë¦¬ ë„êµ¬ (LangChain document parser ê¸°ë°˜)
    PDF íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì²­í‚¹
    """

    name = "pdf_processor"
    description = "PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì²­í‚¹í•©ë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”."

    def _run(self, file_path: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """
        PDF ì²˜ë¦¬ ì‹¤í–‰
        """
        try:
            logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {file_path}")

            # LangChain PDF ë¡œë”ë“¤
            from langchain_community.document_loaders import PyPDFLoader
            from langchain.text_splitter import RecursiveCharacterTextSplitter
            from pathlib import Path

            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            pdf_path = Path(file_path)
            if not pdf_path.exists():
                return f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}"

            if not pdf_path.suffix.lower() == '.pdf':
                return "PDF íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤."

            # PDF ë¡œë” ì‚¬ìš©
            loader = PyPDFLoader(str(pdf_path))
            documents = loader.load()

            if not documents:
                return "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # í…ìŠ¤íŠ¸ ì²­í‚¹
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", ". ", " ", ""]
            )
            chunks = text_splitter.split_documents(documents)

            # ê²°ê³¼ ìƒì„±
            total_pages = len(documents)
            total_chunks = len(chunks)
            total_chars = sum(len(doc.page_content) for doc in documents)

            result = f"PDF ì²˜ë¦¬ ì™„ë£Œ: {pdf_path.name}\n"
            result += f"- ì´ í˜ì´ì§€: {total_pages}í˜ì´ì§€\n"
            result += f"- ì¶”ì¶œ í…ìŠ¤íŠ¸: {total_chars:,} ë¬¸ì\n"
            result += f"- ìƒì„± ì²­í¬: {total_chunks}ê°œ\n\n"

            # ìƒ˜í”Œ ì²­í¬
            if chunks:
                sample = chunks[0].page_content[:200] + "..." if len(chunks[0].page_content) > 200 else chunks[0].page_content
                result += f"ìƒ˜í”Œ ì²­í¬:\n{sample}\n"

            return result

        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def _arun(self, file_path: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """
        PDF ì²˜ë¦¬ ë¹„ë™ê¸° ì‹¤í–‰
        """
        return self._run(file_path, run_manager)


class MemoryAccessTool(BaseTool):
    """
    ë©”ëª¨ë¦¬ ì ‘ê·¼ Tool
    LangChain Memoryì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    """

    name = "memory_access"
    description = "ëŒ€í™” ë©”ëª¨ë¦¬ì—ì„œ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."

    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """
        ë™ê¸° ë©”ëª¨ë¦¬ ì ‘ê·¼
        """
        try:
            logger.info("ğŸ§  ë©”ëª¨ë¦¬ ì ‘ê·¼")

            from ..memory import get_conversation_memory

            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ì¸ìŠ¤í„´ìŠ¤
            memory = get_conversation_memory()

            # ë©”ëª¨ë¦¬ ë³€ìˆ˜ ë¡œë“œ
            memory_vars = memory.load_memory_variables({})

            history = memory_vars.get("history", "")

            if history:
                return f"ëŒ€í™” íˆìŠ¤í† ë¦¬:\n{history}"
            else:
                return "ì €ì¥ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."

        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return f"ë©”ëª¨ë¦¬ ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def _arun(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """
        ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì ‘ê·¼
        """
        return self._run(query, run_manager)


# RAG Tool íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
def create_vector_search_tool() -> VectorSearchTool:
    """ë²¡í„° ê²€ìƒ‰ Tool ìƒì„±"""
    return VectorSearchTool()


def create_document_chunker_tool() -> DocumentChunkerTool:
    """ë¬¸ì„œ ì²­í‚¹ Tool ìƒì„±"""
    return DocumentChunkerTool()


def create_context_retriever_tool() -> ContextRetrieverTool:
    """ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ Tool ìƒì„±"""
    return ContextRetrieverTool()


def create_memory_access_tool() -> MemoryAccessTool:
    """ë©”ëª¨ë¦¬ ì ‘ê·¼ Tool ìƒì„±"""
    return MemoryAccessTool()


# ëª¨ë“  RAG Tool ìƒì„± í•¨ìˆ˜
RAG_TOOL_FACTORIES = {
    "vector_search": create_vector_search_tool,
    "document_chunker": create_document_chunker_tool,
    "context_retriever": create_context_retriever_tool,
    "memory_access": create_memory_access_tool
}


def get_all_rag_tools() -> Dict[str, BaseTool]:
    """
    ëª¨ë“  RAG Tools ìƒì„±

    Returns:
        Tool ì´ë¦„ -> Tool ì¸ìŠ¤í„´ìŠ¤ ë§¤í•‘
    """
    tools = {}
    for name, factory in RAG_TOOL_FACTORIES.items():
        try:
            tools[name] = factory()
            logger.info(f"âœ… RAG Tool ìƒì„±: {name}")
        except Exception as e:
            logger.error(f"âŒ RAG Tool ìƒì„± ì‹¤íŒ¨ ({name}): {e}")

    return tools


def register_rag_tools_to_registry():
    """
    RAG Toolsë¥¼ Tool Registryì— ë“±ë¡
    """
    from .registry import get_tool_registry

    registry = get_tool_registry()
    tools = get_all_rag_tools()

    for name, tool in tools.items():
        registry.register_tool(name, type(tool), instantiate=False)
        registry._tools[name] = tool  # ì§ì ‘ ì¸ìŠ¤í„´ìŠ¤ ë“±ë¡

    logger.info(f"ğŸ“‹ RAG Tools ë“±ë¡ ì™„ë£Œ: {len(tools)}ê°œ")

