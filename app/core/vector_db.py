"""
FAISS ê¸°ë°˜ Vector Database
ë°˜ë„ì²´ ë„ë©”ì¸ ë¬¸ì„œì˜ ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ FAISS êµ¬í˜„
"""
import os
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore

logger = logging.getLogger(__name__)

# FAISS GPU ê²€ì‚¬
try:
    GPU_AVAILABLE = hasattr(faiss, 'StandardGpuResources') and faiss.get_num_gpus() > 0
    if GPU_AVAILABLE:
        logger.info(f" FAISS GPU ì‚¬ìš© ê°€ëŠ¥ ({faiss.get_num_gpus()}ê°œ GPU ê°ì§€)")
    else:
        logger.info(" FAISS CPU ëª¨ë“œ")
except Exception as e:
    GPU_AVAILABLE = False
    logger.warning(f" FAISS GPU ì²´í¬ ì‹¤íŒ¨, CPU ëª¨ë“œë¡œ ì‹¤í–‰: {e}")

from ..memory.conversation_simple import SimpleConversationMemory

logger = logging.getLogger(__name__)


class SentenceTransformerEmbeddings:
    """LangChain í˜¸í™˜ SentenceTransformer ì„ë² ë”© ë˜í¼"""

    def __init__(self, model):
        self.model = model

    def __call__(self, texts: List[str]) -> List[List[float]]:
        """FAISS í˜¸í™˜ í˜¸ì¶œ ë©”ì†Œë“œ"""
        return self.embed_documents(texts)

    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        return self.model.encode(text).tolist()

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        return self.model.encode(texts).tolist()


class FAISSVectorDB:
    """
    FAISS ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ (GPU ìµœì í™”)
    - Sentence Transformers ì„ë² ë”© ì‚¬ìš©
    - GPU ê°€ì† ì¸ë±ì‹± (IVF-PQ, Flat)
    - ê³ ì„±ëŠ¥ ìœ ì‚¬ë„ ê²€ìƒ‰
    """

    def __init__(
        self,
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2", 
        index_path: str = "app/data/vectorstore/faiss_index",
        persist_directory: str = "app/data/vectorstore",
        index_type: str = "auto",  # "auto", "flat", "ivf_pq", "hnsw"
        use_gpu: bool = True
    ):
        self.embedding_model = embedding_model
        self.index_path = Path(index_path)
        self.persist_directory = Path(persist_directory)
        self.persist_directory.mkdir(parents=True, exist_ok=True)
        self.index_type = index_type
        self.use_gpu = use_gpu and GPU_AVAILABLE

        # Sentence Transformer ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings_model = SentenceTransformer(embedding_model)
        self.embedding_dim = self.embeddings_model.get_sentence_embedding_dimension()

        # LangChain í˜¸í™˜ ì„ë² ë”© ë˜í¼
        self.embeddings = SentenceTransformerEmbeddings(self.embeddings_model)

        # FAISS ë²¡í„° ìŠ¤í† ì–´
        self.vectorstore: Optional[FAISS] = None

        # GPU ë¦¬ì†ŒìŠ¤ (GPU ì‚¬ìš©ì‹œ)
        self.gpu_resource = None
        if self.use_gpu:
            try:
                self.gpu_resource = faiss.StandardGpuResources()
                logger.info("ğŸš€ FAISS GPU ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ GPU ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨, CPU ëª¨ë“œë¡œ ì „í™˜: {e}")
                self.use_gpu = False

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self.metadata_file = self.persist_directory / "metadata.json"

        logger.info(f"ğŸ¯ FAISS VectorDB ì´ˆê¸°í™”: {embedding_model} ({'GPU' if self.use_gpu else 'CPU'} ëª¨ë“œ, {self.index_type} ì¸ë±ìŠ¤)")

    def initialize(self) -> bool:
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            if self._load_index():
                logger.info("âœ… ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ")
                return True

            # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
            self._create_empty_index()
            logger.info("âœ… ìƒˆ FAISS ì¸ë±ìŠ¤ ìƒì„±")
            return True

        except Exception as e:
            logger.error(f"âŒ FAISS VectorDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def _create_empty_index(self):
        """ë¹ˆ FAISS ì¸ë±ìŠ¤ ìƒì„± (GPU ìµœì í™”, ë‹¤ì–‘í•œ ì¸ë±ìŠ¤ íƒ€ì… ì§€ì›)"""
        # ì¸ë±ìŠ¤ íƒ€ì… ê²°ì •
        if self.index_type == "auto":
            # ìë™ ì„ íƒ: GPU ì‚¬ìš© ì‹œ Flat, ì•„ë‹ˆë©´ IVF-PQ
            index_type = "flat" if self.use_gpu else "ivf_pq"
        else:
            index_type = self.index_type

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        if index_type == "flat":
            # Inner Product for cosine similarity (L2 ì •ê·œí™” ì„ë² ë”© í•„ìš”)
            index = faiss.IndexFlatIP(self.embedding_dim)
            logger.info("ğŸ“ Flat ì¸ë±ìŠ¤ (ì •í™•í•œ ê²€ìƒ‰, ë¹ ë¥¸ ì†Œê·œëª¨ DB)")
            
        elif index_type == "ivf_pq":
            # IVF-PQ ì¸ë±ìŠ¤ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ëŒ€ìš©ëŸ‰ DBìš©)
            nlist = min(100, max(4, int(np.sqrt(10000))))  # í´ëŸ¬ìŠ¤í„° ìˆ˜ (ìµœì†Œ 4, ìµœëŒ€ 100)
            m = 8        # PQ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
            nbits = 8    # ë¹„íŠ¸ ìˆ˜
            quantizer = faiss.IndexFlatIP(self.embedding_dim)
            index = faiss.IndexIVFPQ(quantizer, self.embedding_dim, nlist, m, nbits)
            # IVF ì¸ë±ìŠ¤ëŠ” trainì´ í•„ìš”í•˜ì§€ë§Œ ë¹ˆ ìƒíƒœì—ì„œëŠ” ìŠ¤í‚µ
            logger.info(f"ğŸ—‚ï¸ IVF-PQ ì¸ë±ìŠ¤ (ë©”ëª¨ë¦¬ íš¨ìœ¨, nlist={nlist})")
            
        elif index_type == "hnsw":
            # HNSW ì¸ë±ìŠ¤ (ë¹ ë¥¸ ê·¼ì‚¬ ê²€ìƒ‰)
            M = 32  # ì—°ê²° ìˆ˜
            index = faiss.IndexHNSWFlat(self.embedding_dim, M)
            index.hnsw.efConstruction = 200  # êµ¬ì¶• ì‹œ íƒìƒ‰ ê¹Šì´
            index.hnsw.efSearch = 100        # ê²€ìƒ‰ ì‹œ íƒìƒ‰ ê¹Šì´
            logger.info(f"ğŸ•¸ï¸ HNSW ì¸ë±ìŠ¤ (ë¹ ë¥¸ ê·¼ì‚¬ ê²€ìƒ‰, M={M})")
            
        else:
            # ê¸°ë³¸ê°’: Flat
            index = faiss.IndexFlatIP(self.embedding_dim)
            logger.info("ğŸ“ ê¸°ë³¸ Flat ì¸ë±ìŠ¤")

        # GPU ì‚¬ìš© ì‹œ GPUë¡œ ì´ë™
        if self.use_gpu and index_type != "ivf_pq":  # IVF-PQëŠ” GPU ì§€ì› ì œí•œì 
            try:
                gpu_index = faiss.index_cpu_to_gpu(self.gpu_resource, 0, index)
                index = gpu_index
                logger.info(f"ğŸš€ {index_type.upper()} ì¸ë±ìŠ¤ GPUë¡œ ì´ë™ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ GPU ì´ë™ ì‹¤íŒ¨, CPUì—ì„œ ì‹¤í–‰: {e}")

        # LangChain FAISS ë˜í¼ë¡œ ìƒì„±
        self.vectorstore = FAISS(
            embedding_function=self.embeddings,
            index=index,
            docstore=InMemoryDocstore(),
            index_to_docstore_id={}
        )

        logger.info(f"âœ… {index_type.upper()} ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ (ì°¨ì›: {self.embedding_dim}, {'GPU' if self.use_gpu and index_type != 'ivf_pq' else 'CPU'})")

    def _load_index(self) -> bool:
        """ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            if not self.index_path.exists():
                return False

            self.vectorstore = FAISS.load_local(
                str(self.index_path),
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            return True

        except Exception as e:
            logger.warning(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def save_index(self):
        """FAISS ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            if self.vectorstore:
                self.vectorstore.save_local(str(self.index_path))
                logger.info(f"ğŸ’¾ FAISS ì¸ë±ìŠ¤ ì €ì¥: {self.index_path}")
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    def add_documents(self, documents: List[Document]):
        """ë¬¸ì„œ ì¶”ê°€ ë° ì¸ë±ì‹±"""
        try:
            if not self.vectorstore:
                logger.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return False

            # ë¬¸ì„œ ì¶”ê°€
            self.vectorstore.add_documents(documents)

            # ì¸ë±ìŠ¤ ì €ì¥
            self.save_index()

            logger.info(f"âœ… ë¬¸ì„œ {len(documents)}ê°œ ì¶”ê°€ ë° ì¸ë±ì‹± ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

    def add_texts(self, texts: List[str], metadatas: Optional[List[Dict[str, Any]]] = None):
        """í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ê°€"""
        try:
            if not self.vectorstore:
                logger.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return False

            # ë©”íƒ€ë°ì´í„° ê¸°ë³¸ê°’ ì„¤ì •
            if metadatas is None:
                metadatas = [{"source": f"text_{i}", "chunk_id": i} for i in range(len(texts))]

            # í…ìŠ¤íŠ¸ ì¶”ê°€
            self.vectorstore.add_texts(texts, metadatas=metadatas)

            # ì¸ë±ìŠ¤ ì €ì¥
            self.save_index()

            logger.info(f"âœ… í…ìŠ¤íŠ¸ {len(texts)}ê°œ ì¶”ê°€ ë° ì¸ë±ì‹± ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

    def similarity_search(
        self,
        query: str,
        k: int = 5,
        score_threshold: float = 0.0
    ) -> List[Tuple[Document, float]]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            if not self.vectorstore:
                logger.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return []

            # ìœ ì‚¬ë„ ê²€ìƒ‰ (ì ìˆ˜ í¬í•¨)
            docs_and_scores = self.vectorstore.similarity_search_with_score(
                query,
                k=k
            )

            # ì ìˆ˜ í•„í„°ë§
            filtered_results = [
                (doc, score) for doc, score in docs_and_scores
                if score >= score_threshold
            ]

            logger.info(f"ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ê²°ê³¼ (ì¿¼ë¦¬: {query[:50]}...)")
            return filtered_results

        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def get_stats(self) -> Dict[str, Any]:
        """ë²¡í„° DB í†µê³„"""
        try:
            if not self.vectorstore:
                return {"status": "not_initialized"}

            # ê¸°ë³¸ í†µê³„
            stats = {
                "status": "initialized",
                "embedding_model": self.embedding_model,
                "index_path": str(self.index_path),
                "total_documents": len(self.vectorstore.docstore._dict) if hasattr(self.vectorstore.docstore, '_dict') else 0
            }

            return stats

        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}

    def clear_index(self):
        """ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        try:
            self._create_empty_index()
            self.save_index()
            logger.info("ğŸ§¹ FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_vector_db_instance: Optional[FAISSVectorDB] = None

def get_vector_db() -> FAISSVectorDB:
    """FAISS VectorDB ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
    global _vector_db_instance
    if _vector_db_instance is None:
        _vector_db_instance = FAISSVectorDB()
        _vector_db_instance.initialize()
    return _vector_db_instance

def initialize_sample_data():
    """ìƒ˜í”Œ ë°˜ë„ì²´ ë¬¸ì„œ ë°ì´í„° ì¶”ê°€"""
    vector_db = get_vector_db()

    # ìƒ˜í”Œ ë°˜ë„ì²´ ë¬¸ì„œë“¤
    sample_documents = [
        Document(
            page_content="ë°˜ë„ì²´ ì œì¡° ê³µì •ì€ í¬ê²Œ 8ë‹¨ê³„ë¡œ ë‚˜ë‰©ë‹ˆë‹¤: ì›¨ì´í¼ ì œì¡°, ì‚°í™”, í¬í† ë¦¬ì†Œê·¸ë˜í”¼, ì‹ê°, ì´ì˜¨ì£¼ì…, ê¸ˆì†í™”, íŒ¨ì‹œë² ì´ì…˜, íŒ¨í‚¤ì§•ì…ë‹ˆë‹¤.",
            metadata={"source": "semiconductor_fundamentals.pdf", "chunk_id": 1, "topic": "ì œì¡°ê³µì •"}
        ),
        Document(
            page_content="VirtualFabì€ ë°˜ë„ì²´ ê³µì¥ì„ ê°€ìƒìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” Digital Twin ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê³µì • ìµœì í™”ì™€ í’ˆì§ˆ í–¥ìƒì„ ì‹¤í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            metadata={"source": "virtualfab_guide.pdf", "chunk_id": 2, "topic": "VirtualFab"}
        ),
        Document(
            page_content="Digital Twinì€ ë¬¼ë¦¬ì  ì‹œìŠ¤í…œì˜ ê°€ìƒ ë³µì œë³¸ìœ¼ë¡œ, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ì˜ˆì¸¡ ìµœì í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë°˜ë„ì²´ ì‚°ì—…ì—ì„œ íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤.",
            metadata={"source": "digital_twin_overview.pdf", "chunk_id": 3, "topic": "DigitalTwin"}
        ),
        Document(
            page_content="ë°˜ë„ì²´ 8ëŒ€ ê³µì •: 1) ì›¨ì´í¼ ì œì¡° 2) ì‚°í™”ë§‰ í˜•ì„± 3) í¬í† ë¦¬ì†Œê·¸ë˜í”¼ 4) ì‹ê° 5) ì´ì˜¨ì£¼ì… 6) ê¸ˆì† ë°°ì„  7) íŒ¨ì‹œë² ì´ì…˜ 8) íŒ¨í‚¤ì§•",
            metadata={"source": "process_guide.pdf", "chunk_id": 4, "topic": "8ëŒ€ê³µì •"}
        ),
        Document(
            page_content="VirtualFab í”Œë«í¼ì€ í´ë¼ìš°ë“œ ê¸°ë°˜ ë°˜ë„ì²´ ì„¤ê³„ ë° ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤. AI ê¸°ë°˜ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•©ë‹ˆë‹¤.",
            metadata={"source": "platform_features.pdf", "chunk_id": 5, "topic": "í”Œë«í¼"}
        )
    ]

    # ë¬¸ì„œ ì¶”ê°€
    success = vector_db.add_documents(sample_documents)

    if success:
        logger.info("âœ… ìƒ˜í”Œ ë°˜ë„ì²´ ë¬¸ì„œ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
    else:
        logger.error("âŒ ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨")

    return success
