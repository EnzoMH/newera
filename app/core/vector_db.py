"""
FAISS ê¸°ë°˜ Vector Database
ë°˜ë„ì²´ ë„ë©”ì¸ ë¬¸ì„œì˜ ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ FAISS êµ¬í˜„
"""
import os
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore

from ..memory.conversation_simple import SimpleConversationMemory

logger = logging.getLogger(__name__)


class SentenceTransformerEmbeddings:
    """LangChain í˜¸í™˜ SentenceTransformer ì„ë² ë”© ë˜í¼"""

    def __init__(self, model):
        self.model = model

    def __call__(self, texts: List[str]) -> List[List[float]]:
        """FAISS í˜¸í™˜ í˜¸ì¶œ ë©”ì†Œë“œ"""
        return self.embed_documents(texts)

    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        return self.model.encode(text).tolist()

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        return self.model.encode(texts).tolist()


class FAISSVectorDB:
    """
    FAISS ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
    - Sentence Transformers ì„ë² ë”© ì‚¬ìš©
    - ë¬¸ì„œ ì²­í‚¹ ë° ì¸ë±ì‹±
    - ìœ ì‚¬ë„ ê²€ìƒ‰
    """

    def __init__(
        self,
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        index_path: str = "app/data/vectorstore/faiss_index",
        persist_directory: str = "app/data/vectorstore"
    ):
        self.embedding_model = embedding_model
        self.index_path = Path(index_path)
        self.persist_directory = Path(persist_directory)
        self.persist_directory.mkdir(parents=True, exist_ok=True)

        # Sentence Transformer ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings_model = SentenceTransformer(embedding_model)

        # LangChain í˜¸í™˜ ì„ë² ë”© ë˜í¼
        self.embeddings = SentenceTransformerEmbeddings(self.embeddings_model)

        # FAISS ë²¡í„° ìŠ¤í† ì–´
        self.vectorstore: Optional[FAISS] = None

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self.metadata_file = self.persist_directory / "metadata.json"

        logger.info(f"ğŸ¯ FAISS VectorDB ì´ˆê¸°í™”: {embedding_model}")

    def initialize(self) -> bool:
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            if self._load_index():
                logger.info("âœ… ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ")
                return True

            # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
            self._create_empty_index()
            logger.info("âœ… ìƒˆ FAISS ì¸ë±ìŠ¤ ìƒì„±")
            return True

        except Exception as e:
            logger.error(f"âŒ FAISS VectorDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def _create_empty_index(self):
        """ë¹ˆ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        # ìƒ˜í”Œ ë²¡í„°ë¡œ ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ë‚˜ì¤‘ì— ì‹¤ì œ ë¬¸ì„œë¡œ êµì²´)
        sample_embedding = self.embeddings.embed_query("ìƒ˜í”Œ í…ìŠ¤íŠ¸")
        dimension = len(sample_embedding)

        # FAISS ì¸ë±ìŠ¤ ìƒì„± (ì§ì ‘ ìƒì„±)
        index = faiss.IndexFlatL2(dimension)
        self.vectorstore = FAISS(
            embedding_function=self.embeddings,
            index=index,
            docstore=InMemoryDocstore(),
            index_to_docstore_id={}
        )

        logger.info(f"âœ… ë¹ˆ FAISS ì¸ë±ìŠ¤ ìƒì„± (ì°¨ì›: {dimension})")

    def _load_index(self) -> bool:
        """ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            if not self.index_path.exists():
                return False

            self.vectorstore = FAISS.load_local(
                str(self.index_path),
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            return True

        except Exception as e:
            logger.warning(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def save_index(self):
        """FAISS ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            if self.vectorstore:
                self.vectorstore.save_local(str(self.index_path))
                logger.info(f"ğŸ’¾ FAISS ì¸ë±ìŠ¤ ì €ì¥: {self.index_path}")
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    def add_documents(self, documents: List[Document]):
        """ë¬¸ì„œ ì¶”ê°€ ë° ì¸ë±ì‹±"""
        try:
            if not self.vectorstore:
                logger.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return False

            # ë¬¸ì„œ ì¶”ê°€
            self.vectorstore.add_documents(documents)

            # ì¸ë±ìŠ¤ ì €ì¥
            self.save_index()

            logger.info(f"âœ… ë¬¸ì„œ {len(documents)}ê°œ ì¶”ê°€ ë° ì¸ë±ì‹± ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

    def add_texts(self, texts: List[str], metadatas: Optional[List[Dict[str, Any]]] = None):
        """í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ê°€"""
        try:
            if not self.vectorstore:
                logger.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return False

            # ë©”íƒ€ë°ì´í„° ê¸°ë³¸ê°’ ì„¤ì •
            if metadatas is None:
                metadatas = [{"source": f"text_{i}", "chunk_id": i} for i in range(len(texts))]

            # í…ìŠ¤íŠ¸ ì¶”ê°€
            self.vectorstore.add_texts(texts, metadatas=metadatas)

            # ì¸ë±ìŠ¤ ì €ì¥
            self.save_index()

            logger.info(f"âœ… í…ìŠ¤íŠ¸ {len(texts)}ê°œ ì¶”ê°€ ë° ì¸ë±ì‹± ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

    def similarity_search(
        self,
        query: str,
        k: int = 5,
        score_threshold: float = 0.0
    ) -> List[Tuple[Document, float]]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            if not self.vectorstore:
                logger.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return []

            # ìœ ì‚¬ë„ ê²€ìƒ‰ (ì ìˆ˜ í¬í•¨)
            docs_and_scores = self.vectorstore.similarity_search_with_score(
                query,
                k=k
            )

            # ì ìˆ˜ í•„í„°ë§
            filtered_results = [
                (doc, score) for doc, score in docs_and_scores
                if score >= score_threshold
            ]

            logger.info(f"ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ê²°ê³¼ (ì¿¼ë¦¬: {query[:50]}...)")
            return filtered_results

        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def get_stats(self) -> Dict[str, Any]:
        """ë²¡í„° DB í†µê³„"""
        try:
            if not self.vectorstore:
                return {"status": "not_initialized"}

            # ê¸°ë³¸ í†µê³„
            stats = {
                "status": "initialized",
                "embedding_model": self.embedding_model,
                "index_path": str(self.index_path),
                "total_documents": len(self.vectorstore.docstore._dict) if hasattr(self.vectorstore.docstore, '_dict') else 0
            }

            return stats

        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}

    def clear_index(self):
        """ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        try:
            self._create_empty_index()
            self.save_index()
            logger.info("ğŸ§¹ FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_vector_db_instance: Optional[FAISSVectorDB] = None

def get_vector_db() -> FAISSVectorDB:
    """FAISS VectorDB ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
    global _vector_db_instance
    if _vector_db_instance is None:
        _vector_db_instance = FAISSVectorDB()
        _vector_db_instance.initialize()
    return _vector_db_instance

def initialize_sample_data():
    """ìƒ˜í”Œ ë°˜ë„ì²´ ë¬¸ì„œ ë°ì´í„° ì¶”ê°€"""
    vector_db = get_vector_db()

    # ìƒ˜í”Œ ë°˜ë„ì²´ ë¬¸ì„œë“¤
    sample_documents = [
        Document(
            page_content="ë°˜ë„ì²´ ì œì¡° ê³µì •ì€ í¬ê²Œ 8ë‹¨ê³„ë¡œ ë‚˜ë‰©ë‹ˆë‹¤: ì›¨ì´í¼ ì œì¡°, ì‚°í™”, í¬í† ë¦¬ì†Œê·¸ë˜í”¼, ì‹ê°, ì´ì˜¨ì£¼ì…, ê¸ˆì†í™”, íŒ¨ì‹œë² ì´ì…˜, íŒ¨í‚¤ì§•ì…ë‹ˆë‹¤.",
            metadata={"source": "semiconductor_fundamentals.pdf", "chunk_id": 1, "topic": "ì œì¡°ê³µì •"}
        ),
        Document(
            page_content="VirtualFabì€ ë°˜ë„ì²´ ê³µì¥ì„ ê°€ìƒìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” Digital Twin ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê³µì • ìµœì í™”ì™€ í’ˆì§ˆ í–¥ìƒì„ ì‹¤í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            metadata={"source": "virtualfab_guide.pdf", "chunk_id": 2, "topic": "VirtualFab"}
        ),
        Document(
            page_content="Digital Twinì€ ë¬¼ë¦¬ì  ì‹œìŠ¤í…œì˜ ê°€ìƒ ë³µì œë³¸ìœ¼ë¡œ, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ì˜ˆì¸¡ ìµœì í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë°˜ë„ì²´ ì‚°ì—…ì—ì„œ íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤.",
            metadata={"source": "digital_twin_overview.pdf", "chunk_id": 3, "topic": "DigitalTwin"}
        ),
        Document(
            page_content="ë°˜ë„ì²´ 8ëŒ€ ê³µì •: 1) ì›¨ì´í¼ ì œì¡° 2) ì‚°í™”ë§‰ í˜•ì„± 3) í¬í† ë¦¬ì†Œê·¸ë˜í”¼ 4) ì‹ê° 5) ì´ì˜¨ì£¼ì… 6) ê¸ˆì† ë°°ì„  7) íŒ¨ì‹œë² ì´ì…˜ 8) íŒ¨í‚¤ì§•",
            metadata={"source": "process_guide.pdf", "chunk_id": 4, "topic": "8ëŒ€ê³µì •"}
        ),
        Document(
            page_content="VirtualFab í”Œë«í¼ì€ í´ë¼ìš°ë“œ ê¸°ë°˜ ë°˜ë„ì²´ ì„¤ê³„ ë° ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤. AI ê¸°ë°˜ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•©ë‹ˆë‹¤.",
            metadata={"source": "platform_features.pdf", "chunk_id": 5, "topic": "í”Œë«í¼"}
        )
    ]

    # ë¬¸ì„œ ì¶”ê°€
    success = vector_db.add_documents(sample_documents)

    if success:
        logger.info("âœ… ìƒ˜í”Œ ë°˜ë„ì²´ ë¬¸ì„œ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
    else:
        logger.error("âŒ ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨")

    return success
