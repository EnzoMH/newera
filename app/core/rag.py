"""
RAG System Core
ë‹¨ì¼ ì±…ì„: VirtualFab RAG ì‹œìŠ¤í…œì˜ ì „ì²´ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
"""
import logging
from typing import Dict, List, Optional, Any
import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ config import
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
try:
    from config import MODEL_NAME
except ImportError:
    MODEL_NAME = None

from .llm import OllamaLLMProvider, OllamaRequest
from .llm.dto import OllamaResponse
from .vector_db import get_vector_db, FAISSVectorDB, initialize_sample_data

logger = logging.getLogger(__name__)


class RAGSystem:
    """
    VirtualFab RAG System
    - ë‹¨ì¼ ì±…ì„: ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
    - ë°˜ë„ì²´ ì œì¡°(VirtualFab/Digital Twin) ë„ë©”ì¸ íŠ¹í™”
    """

    def __init__(self):
        self.llm_provider: Optional[OllamaLLMProvider] = None
        self.vector_store: Optional[FAISSVectorDB] = None
        self.crawler = None      # ì¶”í›„ êµ¬í˜„
        self.retriever = None    # ì¶”í›„ êµ¬í˜„

        self.is_initialized = False

        logger.info("ğŸ¯ RAG System ì´ˆê¸°í™” ì¤‘...")

    def initialize(self) -> bool:
        """
        RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”

        Returns:
            ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            # LLM Provider ì´ˆê¸°í™”
            # í™˜ê²½ë³€ìˆ˜ ìš°ì„ ìˆœìœ„: OLLAMA_MODEL > MODEL_NAME (config.py) > ê¸°ë³¸ê°’
            ollama_model = (
                os.getenv("OLLAMA_MODEL") or 
                (MODEL_NAME if MODEL_NAME else None) or 
                "exaone-1.2b:latest"
            )
            logger.info(f"ğŸ¤– ì‚¬ìš©í•  ëª¨ë¸: {ollama_model}")
            self.llm_provider = OllamaLLMProvider(model_name=ollama_model)

            # LLM ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if not self.llm_provider.is_available():
                logger.warning(f"âš ï¸ Ollama LLM ëª¨ë¸ '{ollama_model}'ì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
                logger.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ í™•ì¸í•˜ì„¸ìš”: ollama list")
                return False

            # VectorDB ì´ˆê¸°í™”
            self.vector_store = get_vector_db()
            if not self.vector_store.initialize():
                logger.warning("âš ï¸ VectorDB ì´ˆê¸°í™” ì‹¤íŒ¨. ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                # return False  # VectorDB ì—†ì–´ë„ ê¸°ë³¸ ê¸°ëŠ¥ì€ ë™ì‘ ê°€ëŠ¥

            # ìƒ˜í”Œ ë°ì´í„° ì´ˆê¸°í™” (ì²« ì‹¤í–‰ì‹œ)
            try:
                initialize_sample_data()
                logger.info("âœ… ìƒ˜í”Œ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ ìƒ˜í”Œ ë°ì´í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

            self.is_initialized = True
            logger.info("âœ… RAG System ì´ˆê¸°í™” ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ RAG System ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def query(self, question: str, **kwargs) -> Dict[str, Any]:
        """
        RAG ì§ˆì˜ ì²˜ë¦¬

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°ë“¤

        Returns:
            ì‘ë‹µ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_initialized:
            return {
                "answer": "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "sources": [],
                "metadata": {"error": "not_initialized"}
            }

        try:
            logger.info(f"ğŸ“¥ RAG ì§ˆì˜: {question}")

            # 1. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            search_results = []
            if self.vector_store:
                search_results = self.vector_store.similarity_search(
                    query=question,
                    k=kwargs.get('top_k', 3),
                    score_threshold=kwargs.get('score_threshold', 0.0)
                )
                logger.info(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ë¬¸ì„œ")

            # 2. ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = ""
            sources = []
            if search_results:
                context_parts = []
                for doc, score in search_results:
                    context_parts.append(doc.page_content)
                    sources.append({
                        "content": doc.page_content,
                        "source": doc.metadata.get("source", "unknown"),
                        "score": float(score),
                        "chunk_id": doc.metadata.get("chunk_id", 0)
                    })
                context = "\n\n".join(context_parts)
            else:
                logger.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ LLM ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")

            # 3. RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            rag_prompt = self._get_rag_prompt(question, context)

            # 4. LLM í˜¸ì¶œ
            request = OllamaRequest(
                prompt=rag_prompt,
                system_prompt=self._get_system_prompt(),
                temperature=kwargs.get('temperature', 0.1)
            )

            response = self.llm_provider.generate_response(request)

            return {
                "answer": response.response,
                "sources": sources,
                "metadata": {
                    "llm_provider": "ollama",
                    "model": response.model_name,
                    "rag_enabled": len(sources) > 0,
                    "search_results_count": len(sources),
                    "context_length": len(context)
                }
            }

        except Exception as e:
            logger.error(f"âŒ RAG ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "answer": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": [],
                "metadata": {"error": str(e)}
            }

    def _get_system_prompt(self) -> str:
        """
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        VirtualFab ë„ë©”ì¸ íŠ¹í™”

        Returns:
            ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        """
        return """ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì œì¡°(VirtualFab/Digital Twin) ë„ë©”ì¸ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì „ë¬¸ ë¶„ì•¼:
- ë°˜ë„ì²´ ê³µì • (8ëŒ€ ê³µì •, Lithography, Etching ë“±)
- Virtual Metrology
- Digital Twin
- Predictive Maintenance
- Process Optimization
- Yield Management

ì‘ë‹µ ì›ì¹™:
1. ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ ì œê³µ
2. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…
3. í•„ìš”í•œ ê²½ìš° ì˜ˆì‹œë‚˜ ì¶”ê°€ ì„¤ëª… í¬í•¨
4. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ ë°íˆê³  ì¶”ì •í•˜ì§€ ì•ŠìŒ

ì§ˆë¬¸ì— ì„±ì‹¤í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""

    def _get_rag_prompt(self, question: str, context: str) -> str:
        """
        RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
        ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ì§ˆë¬¸ ë‹µë³€

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸

        Returns:
            RAG í”„ë¡¬í”„íŠ¸
        """
        if context:
            return f"""ë‹¤ìŒì€ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:

{context}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        else:
            return f"ì§ˆë¬¸: {question}\n\në‹µë³€:"

    def get_status(self) -> Dict[str, Any]:
        """
        ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ

        Returns:
            ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
        """
        return {
            "initialized": self.is_initialized,
            "llm_available": self.llm_provider.is_available() if self.llm_provider else False,
            "vector_store_available": self.vector_store is not None,
            "crawler_available": self.crawler is not None,
            "retriever_available": self.retriever is not None,
            "domain": "VirtualFab/Digital Twin"
        }