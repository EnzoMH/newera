"""
LLM Provider (LlamaCpp - Exaone)
ë‹¨ì¼ ì±…ì„: LLMê³¼ì˜ ìƒí˜¸ì‘ìš©
"""
# âš ï¸ ì¤‘ìš”: ë‹¤ë¥¸ import ì „ì— Jinja2 íŒ¨ì¹˜ë¥¼ ë¨¼ì € ì ìš©
try:
    import llama_cpp.llama_chat_format as chat_format
    from jinja2 import Environment
    
    # Jinja2ChatFormatter íŒ¨ì¹˜ (loopcontrols extension í™œì„±í™”)
    OriginalFormatter = chat_format.Jinja2ChatFormatter
    
    class PatchedJinja2ChatFormatter(OriginalFormatter):
        def __init__(self, template, eos_token, bos_token, add_generation_prompt=True, stop_token_ids=None):
            # ì›ë³¸ ì†ì„± ì„¤ì •
            self.template = template
            self.eos_token = eos_token
            self.bos_token = bos_token
            self.add_generation_prompt = add_generation_prompt
            self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None
            
            # loopcontrols extensionì´ í™œì„±í™”ëœ Jinja2 í™˜ê²½ìœ¼ë¡œ í…œí”Œë¦¿ ì»´íŒŒì¼
            from jinja2.sandbox import ImmutableSandboxedEnvironment
            import jinja2
            
            env = ImmutableSandboxedEnvironment(
                loader=jinja2.BaseLoader(),
                extensions=['jinja2.ext.loopcontrols'],  # í•µì‹¬: loopcontrols ì¶”ê°€!
                trim_blocks=True,
                lstrip_blocks=True,
            )
            self._environment = env.from_string(self.template)
    
    chat_format.Jinja2ChatFormatter = PatchedJinja2ChatFormatter
except ImportError:
    # llama-cpp-python ë¯¸ì„¤ì¹˜ ì‹œ ìŠ¤í‚µ
    pass

import logging
from typing import Optional
import os

from app.core.llm.dto import OllamaRequest, OllamaResponse

logger = logging.getLogger(__name__)
logger.info("âœ… Jinja2 loopcontrols extension íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")


class LLMProvider:
    """
    LLM Provider (Ollama ë˜ëŠ” LlamaCpp)
    - ë‹¨ì¼ ì±…ì„: LLMê³¼ì˜ ìƒí˜¸ì‘ìš© ë° ì‘ë‹µ ìƒì„±
    """

    def __init__(self, model_name: str = "LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF"):
        # ìˆœìˆ˜ llama-cpp-python ì‚¬ìš© (LangChain wrapper ë²„ì „ ì¶©ëŒ íšŒí”¼)
        from llama_cpp import Llama

        self.model_name = model_name
        self.filename = os.getenv("LLAMA_CPP_FILENAME", "EXAONE-4.0-1.2B-Q4_K_M.gguf")

        # GPU ë©”ëª¨ë¦¬ ì„¤ì •
        n_gpu_layers = int(os.getenv("LLAMA_CPP_N_GPU_LAYERS", "35"))  # GPU ì‚¬ìš©
        n_ctx = int(os.getenv("LLAMA_CPP_N_CTX", "4096"))  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
        n_batch = int(os.getenv("LLAMA_CPP_N_BATCH", "512"))  # ë°°ì¹˜ í¬ê¸°

        logger.info(f"ğŸ”„ Exaone ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª‡ ë¶„ ì†Œìš”)")
        logger.info(f"   - Repo: {self.model_name}")
        logger.info(f"   - File: {self.filename}")
        logger.info(f"   - GPU Layers: {n_gpu_layers}")

        try:
            # HuggingFace Hubì—ì„œ íŠ¹ì • íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ
            from huggingface_hub import hf_hub_download
            
            logger.info(f"ğŸ“¥ ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘: {self.filename} (ì•½ 700MB)")
            
            # íŠ¹ì • íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ
            model_path = hf_hub_download(
                repo_id=self.model_name,
                filename=self.filename,
                cache_dir="models/exaone"  # ë¡œì»¬ ìºì‹œ
            )
            
            logger.info(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path}")
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ë¡œ ëª¨ë¸ ë¡œë“œ
            self.llm = Llama(
                model_path=model_path,
                n_gpu_layers=n_gpu_layers,
                n_ctx=n_ctx,
                n_batch=n_batch,
                verbose=False  # ë¡œê·¸ ê°„ì†Œí™”
            )

            logger.info(f"âœ… Exaone LlamaCpp LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.filename}")
            logger.info(f"   - GPU: {n_gpu_layers} layers, Context: {n_ctx}, Batch: {n_batch}")
            logger.info(f"   - íŒŒì¼ í¬ê¸°: ~700MB (Q4_K_M ì–‘ìí™”)")

        except Exception as e:
            logger.error(f"âŒ Exaone ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error("ğŸ’¡ í•´ê²° ë°©ë²•:")
            logger.error("   1. llama-cpp-python ë²„ì „ í™•ì¸: pip show llama-cpp-python")
            logger.error("   2. GPU ë“œë¼ì´ë²„ í™•ì¸ (CUDA í•„ìš”)")
            logger.error("   3. ëª¨ë¸ íŒŒì¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œ: https://huggingface.co/" + self.model_name)
            raise

    def generate_response(self, request: OllamaRequest) -> OllamaResponse:
        """
        ê¸°ë³¸ ì‘ë‹µ ìƒì„±

        Args:
            request: Ollama ìš”ì²­ ê°ì²´

        Returns:
            Ollama ì‘ë‹µ ê°ì²´
        """
        try:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
            full_prompt = request.prompt

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            if request.system_prompt:
                full_prompt = f"[ì‹œìŠ¤í…œ]\n{request.system_prompt}\n\n[ì§ˆë¬¸]\n{request.prompt}"

            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            if request.context:
                full_prompt = f"[ì»¨í…ìŠ¤íŠ¸]\n{request.context}\n\n{full_prompt}"

            # LLM í˜¸ì¶œ (create_chat_completion API)
            response = self.llm.create_chat_completion(
                messages=[
                    {"role": "system", "content": request.system_prompt or "ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì œì¡° ë¶„ì•¼ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": full_prompt}
                ],
                max_tokens=request.max_tokens or 1024,
                temperature=request.temperature or 0.1,
                top_p=0.9,
                top_k=40,
                repeat_penalty=1.1,
                stop=["[|endofturn|]", "Human:", "User:"]
            )

            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            response_text = response['choices'][0]['message']['content'].strip()

            return OllamaResponse(
                response=response_text,
                model_name=self.model_name
            )

        except Exception as e:
            logger.error(f"Ollama ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return OllamaResponse(
                response=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                model_name=self.model_name
            )

    def generate_simple_response(self, prompt: str, temperature: float = 0.1, max_tokens: int = 1024) -> str:
        """
        ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±

        Args:
            prompt: í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            temperature: ì˜¨ë„ ì„¤ì • (0.0 ~ 1.0)
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜

        Returns:
            ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        try:
            # create_chat_completion API ì‚¬ìš©
            response = self.llm.create_chat_completion(
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                top_k=40,
                repeat_penalty=1.1,
                stop=["[|endofturn|]", "Human:", "User:"]
            )

            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            response_text = response['choices'][0]['message']['content'].strip()
            return response_text

        except Exception as e:
            logger.error(f"LLM ê°„ë‹¨ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def is_available(self) -> bool:
        """
        LLM ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸

        Returns:
            ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
        """
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
            test_response = self.llm.create_chat_completion(
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )
            return bool(test_response)
        except Exception:
            return False


# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
OllamaLLMProvider = LLMProvider