"""
ë‹¤ì–‘í•œ ì†ŒìŠ¤ í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ ë° ê´€ë¦¬
"""
import logging
from typing import Dict, Type, Optional, Any, List
from pathlib import Path

from .base import BaseCrawler
from .arxiv_crawler import ArXivCrawler

logger = logging.getLogger(__name__)


class CrawlerFactory:
    """
    í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ í´ë˜ìŠ¤
    ì†ŒìŠ¤ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    """

    _crawlers: Dict[str, Type[BaseCrawler]] = {
        "arxiv": ArXivCrawler,
        # ì¶”í›„ ì¶”ê°€ ê°€ëŠ¥:
        # "pubmed": PubMedCrawler,
        # "ieee": IEEECrawler,
        # "acm": ACMCrawler,
    }

    @classmethod
    def create_crawler(
        cls,
        source_type: str,
        output_dir: Optional[Path] = None
    ) -> BaseCrawler:
        """
        í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

        Args:
            source_type: ì†ŒìŠ¤ íƒ€ì… ("arxiv", "pubmed" ë“±)
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬

        Returns:
            í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ íƒ€ì…ì¸ ê²½ìš°
        """
        crawler_class = cls._crawlers.get(source_type.lower())
        if not crawler_class:
            available = ", ".join(cls._crawlers.keys())
            raise ValueError(
                f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ íƒ€ì…: {source_type}. "
                f"ì‚¬ìš© ê°€ëŠ¥í•œ íƒ€ì…: {available}"
            )

        logger.info(f"ğŸ­ í¬ë¡¤ëŸ¬ ìƒì„±: {source_type}")
        return crawler_class(output_dir=output_dir)

    @classmethod
    def register_crawler(cls, source_type: str, crawler_class: Type[BaseCrawler]):
        """
        ìƒˆë¡œìš´ í¬ë¡¤ëŸ¬ ë“±ë¡

        Args:
            source_type: ì†ŒìŠ¤ íƒ€ì… ì´ë¦„
            crawler_class: í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
        """
        cls._crawlers[source_type.lower()] = crawler_class
        logger.info(f"ğŸ“ í¬ë¡¤ëŸ¬ ë“±ë¡: {source_type}")

    @classmethod
    def get_available_sources(cls) -> list[str]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ì†ŒìŠ¤ íƒ€ì… ëª©ë¡ ë°˜í™˜

        Returns:
            ì†ŒìŠ¤ íƒ€ì… ë¦¬ìŠ¤íŠ¸
        """
        return list(cls._crawlers.keys())


class MultiSourceCrawler:
    """
    ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë™ì‹œì— í¬ë¡¤ë§í•˜ëŠ” í¬ë¡¤ëŸ¬
    """

    def __init__(self, output_dir: Optional[Path] = None):
        self.output_dir = Path(output_dir) if output_dir else None
        self.factory = CrawlerFactory()

    async def crawl_multiple(
        self,
        sources: List[str],
        **kwargs
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë™ì‹œì— í¬ë¡¤ë§

        Args:
            sources: í¬ë¡¤ë§í•  ì†ŒìŠ¤ íƒ€ì… ë¦¬ìŠ¤íŠ¸
            **kwargs: ê° í¬ë¡¤ëŸ¬ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°

        Returns:
            ì†ŒìŠ¤ë³„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        import asyncio

        results = {}

        # ê° ì†ŒìŠ¤ë³„ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
        tasks = []
        for source in sources:
            try:
                crawler = self.factory.create_crawler(source, self.output_dir)
                task = crawler.crawl(**kwargs)
                tasks.append((source, task))
            except Exception as e:
                logger.error(f"âŒ {source} í¬ë¡¤ëŸ¬ ìƒì„± ì‹¤íŒ¨: {e}")
                results[source] = []

        # ë³‘ë ¬ ì‹¤í–‰
        for source, task in tasks:
            try:
                source_results = await task
                results[source] = source_results
            except Exception as e:
                logger.error(f"âŒ {source} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                results[source] = []

        return results
