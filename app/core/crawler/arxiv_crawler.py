"""
ArXiv ë…¼ë¬¸ í¬ë¡¤ëŸ¬
ArXiv APIë¥¼ ì‚¬ìš©í•œ ë…¼ë¬¸ ê²€ìƒ‰ ë° ë‹¤ìš´ë¡œë“œ
"""
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
import arxiv
import asyncio

from .base import BaseCrawler

logger = logging.getLogger(__name__)


class ArXivCrawler(BaseCrawler):
    """
    ArXiv ë…¼ë¬¸ í¬ë¡¤ëŸ¬
    - ArXiv APIë¥¼ í†µí•œ ë…¼ë¬¸ ê²€ìƒ‰
    - PDF ë‹¤ìš´ë¡œë“œ
    - ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    """

    def __init__(self, output_dir: Optional[Path] = None):
        super().__init__(output_dir)
        self.client = arxiv.Client()

    def get_source_name(self) -> str:
        return "arxiv"

    async def crawl(
        self,
        categories: Optional[List[str]] = None,
        keywords: Optional[List[str]] = None,
        max_results: int = 100,
        sort_by: str = "submittedDate",
        sort_order: str = "descending"
    ) -> List[Dict[str, Any]]:
        """
        ArXiv ë…¼ë¬¸ í¬ë¡¤ë§

        Args:
            categories: ArXiv ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ì˜ˆ: ["cs.AI", "cs.LG"])
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            sort_by: ì •ë ¬ ê¸°ì¤€ ("relevance", "lastUpdatedDate", "submittedDate")
            sort_order: ì •ë ¬ ìˆœì„œ ("ascending", "descending")

        Returns:
            ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            logger.info(f"ğŸ” ArXiv í¬ë¡¤ë§ ì‹œì‘: categories={categories}, keywords={keywords}, max={max_results}")

            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            query = self._build_query(categories, keywords)

            # ArXiv ê²€ìƒ‰ ì‹¤í–‰ (ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ë˜í•‘)
            loop = asyncio.get_event_loop()
            search = await loop.run_in_executor(
                None,
                lambda: arxiv.Search(
                    query=query,
                    max_results=max_results,
                    sort_by=getattr(arxiv.SortCriterion, sort_by),
                    sort_order=getattr(arxiv.SortOrder, sort_order)
                )
            )

            # ê²°ê³¼ ì²˜ë¦¬
            results = []
            for paper in await loop.run_in_executor(None, lambda: list(self.client.results(search))):
                paper_data = {
                    "id": paper.entry_id.split('/')[-1],
                    "title": paper.title,
                    "authors": [author.name for author in paper.authors],
                    "summary": paper.summary,
                    "published": paper.published.isoformat() if paper.published else None,
                    "updated": paper.updated.isoformat() if paper.updated else None,
                    "categories": paper.categories,
                    "pdf_url": paper.pdf_url,
                    "primary_category": paper.primary_category,
                    "doi": paper.doi if hasattr(paper, 'doi') else None,
                }

                # PDF ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­)
                if self.output_dir:
                    await self._download_pdf(paper, paper_data["id"])

                results.append(paper_data)

            logger.info(f"âœ… ArXiv í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ë…¼ë¬¸ ë°œê²¬")
            return results

        except Exception as e:
            logger.error(f"âŒ ArXiv í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise

    def _build_query(self, categories: Optional[List[str]], keywords: Optional[List[str]]) -> str:
        """
        ArXiv ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±

        Args:
            categories: ì¹´í…Œê³ ë¦¬ ëª©ë¡
            keywords: í‚¤ì›Œë“œ ëª©ë¡

        Returns:
            ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´
        """
        query_parts = []

        # ì¹´í…Œê³ ë¦¬ í•„í„°
        if categories:
            cat_query = " OR ".join([f"cat:{cat}" for cat in categories])
            query_parts.append(f"({cat_query})")

        # í‚¤ì›Œë“œ ê²€ìƒ‰
        if keywords:
            keyword_query = " OR ".join([f'"{kw}"' for kw in keywords])
            query_parts.append(f"({keyword_query})")

        if not query_parts:
            # ê¸°ë³¸ê°’: VirtualFab ê´€ë ¨
            query_parts.append('("VirtualFab" OR "Digital Twin" OR "semiconductor")')

        return " AND ".join(query_parts)

    async def _download_pdf(self, paper: arxiv.Result, paper_id: str) -> Optional[Path]:
        """
        ë…¼ë¬¸ PDF ë‹¤ìš´ë¡œë“œ

        Args:
            paper: ArXiv ë…¼ë¬¸ ê°ì²´
            paper_id: ë…¼ë¬¸ ID

        Returns:
            ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ
        """
        if not self.output_dir:
            return None

        try:
            pdf_dir = self.output_dir / "pdfs"
            pdf_dir.mkdir(parents=True, exist_ok=True)

            pdf_path = pdf_dir / f"{paper_id}.pdf"

            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
            if pdf_path.exists():
                logger.debug(f"ğŸ“„ PDF ì´ë¯¸ ì¡´ì¬: {pdf_path}")
                return pdf_path

            # PDF ë‹¤ìš´ë¡œë“œ
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, paper.download_pdf, str(pdf_dir))

            # íŒŒì¼ëª… ë³€ê²½ (ArXivëŠ” ê¸°ë³¸ì ìœ¼ë¡œ "arxiv_id.pdf"ë¡œ ì €ì¥)
            downloaded_file = pdf_dir / f"{paper_id.replace('.', '_')}.pdf"
            if downloaded_file.exists():
                downloaded_file.rename(pdf_path)

            logger.info(f"ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {pdf_path}")
            return pdf_path

        except Exception as e:
            logger.warning(f"âš ï¸ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({paper_id}): {e}")
            return None

