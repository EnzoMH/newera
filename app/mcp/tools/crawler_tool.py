"""
Web Crawler MCP Tool
ArXiv ë…¼ë¬¸ í¬ë¡¤ë§ ê¸°ëŠ¥ ì œê³µ
"""
import asyncio
import logging
from typing import Dict, Any, List
from pathlib import Path

from ..config import MCPConfig

logger = logging.getLogger(__name__)


class WebCrawlerTool:
    """ArXiv ë…¼ë¬¸ í¬ë¡¤ëŸ¬ MCP Tool"""

    def __init__(self, config: MCPConfig):
        self.config = config
        self.tool_config = config.get_tool_config("web_crawler")

    def get_tool_schema(self) -> Dict[str, Any]:
        """MCP Tool ìŠ¤í‚¤ë§ˆ ë°˜í™˜"""
        return {
            "name": "web_crawler",
            "description": "ArXiv ë…¼ë¬¸ ì›¹ í¬ë¡¤ë§ ë° ë‹¤ìš´ë¡œë“œ",
            "inputSchema": {
                "type": "object",
                "properties": {
                    "categories": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "í¬ë¡¤ë§í•  ArXiv ì¹´í…Œê³ ë¦¬ ëª©ë¡",
                        "default": self.tool_config["arxiv_categories"]
                    },
                    "max_papers": {
                        "type": "integer",
                        "description": "ìµœëŒ€ í¬ë¡¤ë§í•  ë…¼ë¬¸ ìˆ˜",
                        "default": self.tool_config["max_papers"]
                    },
                    "keywords": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡",
                        "default": ["VirtualFab", "Digital Twin", "semiconductor"]
                    }
                },
                "required": []
            }
        }

    async def execute(self, arguments: Dict[str, Any]) -> str:
        """Tool ì‹¤í–‰"""
        try:
            categories = arguments.get("categories", self.tool_config["arxiv_categories"])
            max_papers = arguments.get("max_papers", self.tool_config["max_papers"])
            keywords = arguments.get("keywords", ["VirtualFab", "Digital Twin", "semiconductor"])

            logger.info(f"ğŸ•·ï¸ ArXiv í¬ë¡¤ë§ ì‹œì‘: {categories}, ìµœëŒ€ {max_papers}ê°œ")

            # ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§ (ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜)
            results = await self._crawl_arxiv(categories, max_papers, keywords)

            return f"""âœ… ArXiv í¬ë¡¤ë§ ì™„ë£Œ

ğŸ“Š ê²°ê³¼ ìš”ì•½:
- ì¹´í…Œê³ ë¦¬: {', '.join(categories)}
- ìµœëŒ€ ë…¼ë¬¸ ìˆ˜: {max_papers}
- ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords)}
- ë°œê²¬ëœ ë…¼ë¬¸: {len(results)}

ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.tool_config['output_dir']}

ğŸ“ ìƒì„¸ ê²°ê³¼:
{chr(10).join(f"- {paper['title']} ({paper['id']})" for paper in results[:5])}
{f'... ì™¸ {len(results) - 5}ê°œ' if len(results) > 5 else ''}"""

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return f"âŒ ArXiv í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"

    async def _crawl_arxiv(self, categories: List[str], max_papers: int, keywords: List[str]) -> List[Dict[str, Any]]:
        """ArXiv í¬ë¡¤ë§ ì‹¤í–‰"""
        from pathlib import Path
        from ...core.crawler import ArXivCrawler, KeywordFilter

        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        output_dir = Path(self.tool_config["output_dir"])
        crawler = ArXivCrawler(output_dir=output_dir)

        # í¬ë¡¤ë§ ì‹¤í–‰
        results = await crawler.crawl(
            categories=categories,
            keywords=keywords,
            max_results=max_papers
        )

        # í‚¤ì›Œë“œ í•„í„°ë§ (í•„ìš”í•œ ê²½ìš°)
        if keywords:
            results = KeywordFilter.filter_by_keywords(results, keywords)

        # ê²°ê³¼ ì €ì¥
        if results:
            crawler.save_results(results)

        return results