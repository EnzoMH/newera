"""
RAG FastAPI + Gradio Application
"""
import os
import sys
import logging
from contextlib import asynccontextmanager
from pathlib import Path

import gradio as gr
from fastapi import FastAPI

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from app.vecdb.embedding_service import EmbeddingService
from app.vecdb.faiss_manager import FaissManager
from app.vecdb.mongodb_client import MongoDBClient
from app.vecdb.local_storage import LocalJSONStorage
from app.vecdb.retriever import RAGRetriever
from app.chat.services.gemini_service import GeminiService
from app.chat.router.rag_router import router as rag_router

from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)



logger = logging.getLogger(__name__)




# ì „ì—­ RAG ì»´í¬ë„ŒíŠ¸
rag_components = {}


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    logger.info("="*60)
    logger.info("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    logger.info("="*60)
    
    try:
        # 1. Embedding Service
        logger.info("1. Embedding Service ì´ˆê¸°í™”...")
        embedding_model = os.getenv("EMBEDDING_MODEL", "BAAI/bge-large-en-v1.5")
        embedding_service = EmbeddingService(model_name=embedding_model)
        rag_components['embedding_service'] = embedding_service
        
        # 2. Faiss Manager
        logger.info("2. Faiss ì¸ë±ìŠ¤ ë¡œë“œ...")
        
        # ë¡œì»¬ ì¸ë±ìŠ¤ ìš°ì„  í™•ì¸
        local_index_path = "data/local_vecdb/faiss.index"
        default_index_path = os.getenv("FAISS_INDEX_PATH", "data/vectordb/faiss.index")
        
        index_path = local_index_path if Path(local_index_path).exists() else default_index_path
        
        if not Path(index_path).exists():
            logger.warning(f"Faiss ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤: {index_path}")
            logger.warning("scripts/build_vectordb_local.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”")
            faiss_manager = FaissManager(dimension=embedding_service.get_dimension())
        else:
            faiss_manager = FaissManager(dimension=embedding_service.get_dimension())
            faiss_manager.load(index_path)
            logger.info(f"âœ“ Faiss ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {index_path}")
        
        rag_components['faiss_manager'] = faiss_manager
        
        # 3. Metadata Store (MongoDB or LocalJSON)
        logger.info("3. Metadata Store ì´ˆê¸°í™”...")
        
        use_local = os.getenv("USE_LOCAL_STORAGE", "true").lower() == "true"
        
        if use_local or not Path(local_index_path).exists():
            logger.info("   â†’ Local JSON Storage ì‚¬ìš©")
            metadata_store = LocalJSONStorage(storage_dir="data/local_vecdb")
        else:
            logger.info("   â†’ MongoDB ì‚¬ìš©")
            mongodb_uri = os.getenv("MONGODB_URI", "mongodb://localhost:27017/")
            mongodb_database = os.getenv("MONGODB_DATABASE", "semiconductor_rag")
            mongodb_collection = os.getenv("MONGODB_COLLECTION", "documents")
            metadata_store = MongoDBClient(mongodb_uri, mongodb_database, mongodb_collection)
        
        rag_components['metadata_store'] = metadata_store
        
        # 4. RAG Retriever
        logger.info("4. RAG Retriever ì´ˆê¸°í™”...")
        retriever = RAGRetriever(
            faiss_manager=faiss_manager,
            metadata_store=metadata_store,
            embedding_service=embedding_service
        )
        rag_components['retriever'] = retriever
        
        # 5. LLM Services
        logger.info("5. LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”...")
        
        # Gemini (ì„ íƒì‚¬í•­)
        google_api_key = os.getenv("GOOGLE_API_KEY")
        if google_api_key:
            gemini_model = os.getenv("GEMINI_MODEL", "gemini-2.5-pro")
            gemini_service = GeminiService(api_key=google_api_key, model_name=gemini_model)
            rag_components['gemini_service'] = gemini_service
            logger.info(f"   âœ“ Gemini: {gemini_model}")
        else:
            logger.info("   â“˜ GOOGLE_API_KEY ì—†ìŒ - Gemini ë¯¸ì‚¬ìš©")
        
        # Ollama (ê¸°ë³¸)
        try:
            from langchain_ollama import OllamaLLM
            ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            ollama_model = os.getenv("MODEL_NAME", "exaone-1.2b:latest")
            
            ollama_service = OllamaLLM(
                model=ollama_model,
                base_url=ollama_base_url,
                temperature=0.3,  # ë” ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ì¶¤
                num_ctx=4096,  # Context ê¸¸ì´ ì¦ê°€
            )
            rag_components['ollama_service'] = ollama_service
            logger.info(f"   âœ“ Ollama: {ollama_model}")
        except Exception as e:
            logger.warning(f"   âš  Ollama ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        logger.info("="*60)
        logger.info("âœ“ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        
        stats = retriever.get_stats()
        logger.info(f"í†µê³„:")
        logger.info(f"  - ë²¡í„°: {stats['faiss']['total_vectors']}ê°œ")
        logger.info(f"  - ë¬¸ì„œ: {stats['metadata_store']['total_documents']}ê°œ")
        logger.info(f"  - GPU: {'âœ“' if stats['faiss']['gpu_enabled'] else 'âœ—'}")
        logger.info("="*60)
        
        yield
        
    except Exception as e:
        logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        raise
    
    finally:
        logger.info("RAG ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
        if 'metadata_store' in rag_components:
            rag_components['metadata_store'].close()
        logger.info("âœ“ RAG ì‹œìŠ¤í…œ ì¢…ë£Œ ì™„ë£Œ")


# FastAPI ì•±
app = FastAPI(
    title="Semiconductor RAG System",
    version="1.0.0",
    lifespan=lifespan
)

# RAG Router ë“±ë¡
app.include_router(rag_router, tags=["RAG"])


# Gradio UI
def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    def chat_with_rag(message: str, history: list) -> str:
        """RAG ê¸°ë°˜ ì±„íŒ…"""
        try:
            if not message or message.strip() == "":
                return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            retriever = rag_components.get('retriever')
            gemini_service = rag_components.get('gemini_service')
            ollama_service = rag_components.get('ollama_service')
            
            if not retriever:
                return "âŒ RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            # LLM ìš°ì„ ìˆœìœ„: Ollama(EXAONE) > Gemini
            llm_service = ollama_service if ollama_service else gemini_service
            llm_name = "Ollama (EXAONE)" if ollama_service else "Gemini"
            
            if not llm_service:
                return "âŒ LLM ì„œë¹„ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. Ollamaë¥¼ ì‹¤í–‰í•˜ê±°ë‚˜ GOOGLE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”."
            
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            results = retriever.retrieve(message, top_k=3)
            
            if not results:
                return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
            
            # 2. Context êµ¬ì„±
            context_parts = []
            for i, result in enumerate(results, 1):
                context_parts.append(f"### ë¬¸ì„œ {i}")
                context_parts.append(f"- ì¶œì²˜: {result['metadata']['paper_filename']}")
                context_parts.append(f"- ë„ë©”ì¸: {result['metadata']['domain']}")
                context_parts.append(f"- ìœ ì‚¬ë„: {result['score']:.2f}")
                context_parts.append(f"\n**ë‚´ìš©:**")
                # ë” ë§ì€ context ì œê³µ (500 -> 800ì)
                context_parts.append(f"{result['content'][:800]}")
                if len(result['content']) > 800:
                    context_parts.append("...")
                context_parts.append("")
            
            context = "\n".join(context_parts)
            
            # 3. Prompt êµ¬ì„±
            prompt = f"""ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì œì¡° ë° ì„¤ë¹„ ë¶„ì•¼ì˜ ì „ë¬¸ ì—°êµ¬ì›ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ì°¸ê³  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ **í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ìƒì„¸í•˜ê²Œ** ë‹µë³€í•˜ì„¸ìš”.

## ë‹µë³€ ì›ì¹™
1. ë°˜ë“œì‹œ **í•œêµ­ì–´**ë¡œ ë‹µë³€í•  ê²ƒ
2. ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•  ê²ƒ
3. ê¸°ìˆ ì  ìš©ì–´ëŠ” í•œê¸€ë¡œ ì„¤ëª…í•˜ë˜, í•„ìš”ì‹œ ì˜ë¬¸ ë³‘ê¸° (ì˜ˆ: ìˆ˜ìœ¨(Yield))
4. êµ¬ì²´ì ì¸ ë°©ë²•ë¡ ì´ë‚˜ ê¸°ìˆ ì´ ìˆë‹¤ë©´ ëª…í™•íˆ ì„¤ëª…í•  ê²ƒ
5. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•  ê²ƒ

## ì°¸ê³  ë¬¸ì„œ
{context}

## ì§ˆë¬¸
{message}

## ë‹µë³€ (í•œêµ­ì–´ë¡œ ì‘ì„±):
"""
            
            # 4. LLM í˜¸ì¶œ
            if ollama_service:
                # Ollama(EXAONE) ìš°ì„  ì‚¬ìš©
                response = ollama_service.invoke(prompt)
            else:
                # GeminiëŠ” generate ë©”ì„œë“œ ì‚¬ìš©
                response = gemini_service.generate(prompt)
            
            # 5. ì¶œì²˜ ì¶”ê°€
            sources = f"\n\n---\n**LLM:** {llm_name}\n**ì°¸ê³  ë¬¸ì„œ:**\n"
            for i, result in enumerate(results, 1):
                sources += f"{i}. {result['metadata']['paper_filename']} (ìœ ì‚¬ë„: {result['score']:.2f})\n"
            
            return response + sources
            
        except Exception as e:
            logger.error(f"ì±„íŒ… ì˜¤ë¥˜: {e}", exc_info=True)
            return f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    # Gradio ChatInterface
    demo = gr.ChatInterface(
        fn=chat_with_rag,
        title="ğŸ”¬ ë°˜ë„ì²´ RAG ì‹œìŠ¤í…œ",
        description="""
        ë°˜ë„ì²´ ì œì¡° ë° ì„¤ë¹„ì— ê´€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.
        ArXiv ë…¼ë¬¸ê³¼ ê¸°ìˆ  ë¸”ë¡œê·¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
        
        **ì˜ˆì‹œ ì§ˆë¬¸:**
        - What is Virtual Metrology in semiconductor manufacturing?
        - Explain HNSW algorithm for vector search
        - How does defect inspection work in fab?
        """,
        examples=[
            "What is Virtual Metrology?",
            "Explain fab scheduling optimization",
            "How does defect inspection work?",
            "What are the challenges in semiconductor manufacturing?",
        ],
        theme=gr.themes.Soft(),
    )
    
    return demo


# Gradioë¥¼ FastAPIì— ë§ˆìš´íŠ¸
gradio_app = create_gradio_interface()
app = gr.mount_gradio_app(app, gradio_app, path="/")


@app.get("/health")
async def health():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "rag_initialized": bool(rag_components.get('retriever')),
        "gemini_available": bool(rag_components.get('gemini_service')),
        "ollama_available": bool(rag_components.get('ollama_service')),
        "llm_available": bool(rag_components.get('gemini_service') or rag_components.get('ollama_service')),
    }


if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", "8001"))
    
    logger.info(f"ğŸš€ RAG ì‹œìŠ¤í…œ ì‹œì‘ ì¤‘... (Port: {port})")
    logger.info(f"   - API: http://localhost:{port}/docs")
    logger.info(f"   - Gradio UI: http://localhost:{port}/")
    
    uvicorn.run(
        "rag_main:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        log_level="info"
    )
