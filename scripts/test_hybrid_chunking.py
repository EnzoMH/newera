"""
í•˜ì´ë¸Œë¦¬ë“œ Agentic Chunking í…ŒìŠ¤íŠ¸
- ë¡œì»¬ LLM (Qwen2.5-3B-Korean) vs Gemini 2.0 Flash
- ë¹„ìš©/ì„±ëŠ¥ ë¹„êµ
"""
import sys
from pathlib import Path
import logging
import os
import json

sys.path.insert(0, str(Path(__file__).parent.parent))

from app.vecdb.hybrid_agentic_chunker import HybridAgenticChunker, LLMBackend

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)


def test_hybrid_chunking():
    """í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ í…ŒìŠ¤íŠ¸"""
    
    # ìƒ˜í”Œ ArXiv ë…¼ë¬¸ í…ìŠ¤íŠ¸ ë¡œë“œ
    chunks_dir = Path("data/chunks")
    chunk_files = list(chunks_dir.glob("chunks_*.json"))
    
    if not chunk_files:
        logger.error("ì²­í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        logger.info("ë¨¼ì € scripts/preprocess_pdfs.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
        return
    
    # ì²« ë²ˆì§¸ ë…¼ë¬¸ ì‚¬ìš©
    with open(chunk_files[0], 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ì²­í¬ ë‚´ìš© í•©ì³ì„œ ì›ë³¸ í…ìŠ¤íŠ¸ ì¬êµ¬ì„± (ì¼ë¶€ë§Œ)
    chunks = data.get('chunks', [])[:10]  # ì²˜ìŒ 10ê°œë§Œ
    sample_text = '\n\n'.join([c['content'] for c in chunks])
    
    print("="*80)
    print("í•˜ì´ë¸Œë¦¬ë“œ Agentic Chunking í…ŒìŠ¤íŠ¸")
    print("="*80)
    print(f"\ní…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {data['filename']}")
    print(f"ë„ë©”ì¸: {data['domain']}")
    print(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sample_text):,} chars")
    print()
    
    # í•˜ì´ë¸Œë¦¬ë“œ Chunker ìƒì„±
    try:
        chunker = HybridAgenticChunker(
            local_model="MyeongHo0621/Qwen2.5-3B-Korean",
            local_model_file="gguf/qwen25-3b-korean-Q4_K_M.gguf",
            gemini_api_key=os.getenv("GOOGLE_API_KEY"),
            backend=LLMBackend.AUTO,  # ë¡œì»¬ â†’ Gemini fallback
            use_gpu=True
        )
    except Exception as e:
        logger.error(f"Chunker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.info("\nì„¤ì¹˜ ê°€ì´ë“œ:")
        logger.info("1. llama-cpp-python: pip install llama-cpp-python")
        logger.info("2. GOOGLE_API_KEY ì„¤ì •: export GOOGLE_API_KEY=your_key")
        return
    
    # ì²­í‚¹ ì‹¤í–‰
    print("\n" + "="*80)
    print("ì²­í‚¹ ì‹¤í–‰ ì¤‘...")
    print("="*80)
    
    result_chunks = chunker.chunk_text(sample_text, max_chunks=10)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nìƒì„±ëœ ì²­í¬: {len(result_chunks)}ê°œ")
    print()
    
    for chunk in result_chunks:
        print(f"\n[ì²­í¬ {chunk['id']}]")
        print(f"  LLM ë°±ì—”ë“œ: {chunk.get('llm_backend', 'unknown')}")
        print(f"  LLM ëª¨ë¸: {chunk.get('llm_model', 'unknown')}")
        print(f"  í¬ê¸°: {chunk['size']} chars")
        print(f"  ë‚´ìš© (ì²˜ìŒ 200ì):")
        print(f"  {chunk['content'][:200]}...")
    
    # í†µê³„ ì¶œë ¥
    print()
    chunker.print_stats()
    
    # ë¹„ìš© ë¶„ì„
    stats = chunker.get_stats()
    print("\n" + "="*80)
    print("ë¹„ìš© ë¶„ì„")
    print("="*80)
    
    if stats['local_success'] > 0:
        print(f"âœ… ë¡œì»¬ LLM ì„±ê³µë¥ : {stats['local_success_rate']:.1%}")
        print(f"  - ì ˆê° ë¹„ìš©: ${stats['local_success'] * 0.01:.4f}")
    
    if stats['gemini_success'] > 0:
        print(f"ğŸ’° Gemini ì‚¬ìš©: {stats['gemini_success']}íšŒ")
        print(f"  - ì‹¤ì œ ë¹„ìš©: ${stats['total_cost_usd']:.4f}")
    
    total_docs = stats['total_attempts']
    if total_docs > 0:
        full_gemini_cost = total_docs * 0.01
        saved = full_gemini_cost - stats['total_cost_usd']
        saved_percent = (saved / full_gemini_cost * 100) if full_gemini_cost > 0 else 0
        
        print(f"\ní•˜ì´ë¸Œë¦¬ë“œ íš¨ê³¼:")
        print(f"  - Geminië§Œ ì‚¬ìš© ì‹œ: ${full_gemini_cost:.4f}")
        print(f"  - í•˜ì´ë¸Œë¦¬ë“œ ì‚¬ìš© ì‹œ: ${stats['total_cost_usd']:.4f}")
        print(f"  - ì ˆê°: ${saved:.4f} ({saved_percent:.1f}%)")
    
    print("\n" + "="*80)
    print("âœ“ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("="*80)


if __name__ == "__main__":
    test_hybrid_chunking()

